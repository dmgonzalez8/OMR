{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54478133-a803-4297-96db-2ce4e37e66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import SGD, Adam, Adadelta\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "import torchvision\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
    "import random\n",
    "from math import radians, cos, sin\n",
    "import cv2\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb7df7-c953-4196-aed7-6b23c6120305",
   "metadata": {},
   "source": [
    "### load json annotation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d8b474-7cd3-4ee5-ae9b-c2bb64b1a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data into a dictionary\n",
    "with open('./data/ds2_dense/deepscores_train.json') as file:\n",
    "    data1 = json.load(file)\n",
    "with open('./data/ds2_dense/deepscores_test.json') as file:\n",
    "    data2 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbc3a6e-d40e-48d1-85df-d45a0599839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "# train_labels = pd.DataFrame( data1['categories']).T\n",
    "train_images = pd.DataFrame( data1['images'])\n",
    "train_obboxs = pd.DataFrame( data1['annotations']).T\n",
    "# test_labels = pd.DataFrame( data2['categories']).T\n",
    "test_images = pd.DataFrame( data2['images'])\n",
    "test_obboxs = pd.DataFrame( data2['annotations']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99470ab5-38d9-4d5a-84ab-c0de5588f0a7",
   "metadata": {},
   "source": [
    "### prepare the labels - I adjusted the json slightly so use `new_labels.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3db138a-7b4a-4c80-b6e5-9d901c1fb184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_index</th>\n",
       "      <th>old_id</th>\n",
       "      <th>name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>color</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>brace</td>\n",
       "      <td>deepscores</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>brace</td>\n",
       "      <td>muscima++</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>ledgerLine</td>\n",
       "      <td>deepscores</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>138</td>\n",
       "      <td>ledgerLine</td>\n",
       "      <td>muscima++</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>repeatDot</td>\n",
       "      <td>deepscores</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   old_index  old_id        name     dataset  color  label\n",
       "0          0       1       brace  deepscores      1      1\n",
       "1          1     137       brace   muscima++      1      1\n",
       "2          2       2  ledgerLine  deepscores      2      2\n",
       "3          3     138  ledgerLine   muscima++      2      2\n",
       "4          4       3   repeatDot  deepscores      7      3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the new mapping\n",
    "raw_labels = pd.read_csv('new_labels.csv')\n",
    "raw_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53708547-3aca-4cf6-8f25-36a26e882321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>brace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ledgerLine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>repeatDot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>segno</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        name\n",
       "0      1       brace\n",
       "1      2  ledgerLine\n",
       "2      3   repeatDot\n",
       "3      4       segno"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a df of the unique labels with their names\n",
    "unique_labels = raw_labels[['label', 'name']]\n",
    "unique_labels = unique_labels.drop_duplicates(subset=['label'])\n",
    "unique_labels = unique_labels.sort_values(by=['label']).reset_index(drop=True)\n",
    "unique_labels.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a5fb0-fb57-498c-953d-0170becc754d",
   "metadata": {},
   "source": [
    "### prepare the images/annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ded83a7-c57e-4b8f-93c7-20299d52683e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>ann_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>lg-75827152-aug-lilyjazz-.png</td>\n",
       "      <td>1960</td>\n",
       "      <td>2772</td>\n",
       "      <td>[160131, 160132, 160133, 160134, 160135, 16013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>lg-210359136-aug-lilyjazz--page-14.png</td>\n",
       "      <td>1960</td>\n",
       "      <td>2772</td>\n",
       "      <td>[503778, 503779, 503780, 503781, 503782, 50378...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>lg-366136986510816260-aug-gutenberg1939-.png</td>\n",
       "      <td>1960</td>\n",
       "      <td>2772</td>\n",
       "      <td>[769765, 769766, 769767, 769768, 769769, 76977...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   img_id                                      filename  width  height  \\\n",
       "0       1                 lg-75827152-aug-lilyjazz-.png   1960    2772   \n",
       "1       5        lg-210359136-aug-lilyjazz--page-14.png   1960    2772   \n",
       "2       6  lg-366136986510816260-aug-gutenberg1939-.png   1960    2772   \n",
       "\n",
       "                                             ann_ids  \n",
       "0  [160131, 160132, 160133, 160134, 160135, 16013...  \n",
       "1  [503778, 503779, 503780, 503781, 503782, 50378...  \n",
       "2  [769765, 769766, 769767, 769768, 769769, 76977...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.rename(columns={'id': 'img_id'}, inplace=True)\n",
    "test_images.rename(columns={'id': 'img_id'}, inplace=True)\n",
    "test_images.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c30e0-c9ce-4887-8ddd-bef030ccf604",
   "metadata": {},
   "source": [
    "#### the data has multiple labels for each object but we only need one. some of the object classes were redundant, so I mapped these to the same label in the new_labels.csv file. Some of the labels are very similar but have slightly different contextual information, so in cases where this occurs, we will select the label with the most information. I have set ordered the raw_labels.csv file so that the higher precedence item has a higher label value. Therefore, all we need to do is select the label with the highest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0495705-f92d-4a26-9970-e8f517b1839c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_bbox</th>\n",
       "      <th>o_bbox</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>area</th>\n",
       "      <th>img_id</th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>[116.0, 139.0, 2315.0, 206.0]</td>\n",
       "      <td>[2315.0, 206.0, 2315.0, 139.0, 116.0, 139.0, 1...</td>\n",
       "      <td>[135, 208]</td>\n",
       "      <td>18945</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#000010;</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>[116.0, 309.0, 2315.0, 376.0]</td>\n",
       "      <td>[2315.0, 376.0, 2315.0, 309.0, 116.0, 309.0, 1...</td>\n",
       "      <td>[135, 208]</td>\n",
       "      <td>19223</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#000021;</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>[1880.0, 561.0, 1911.0, 564.0]</td>\n",
       "      <td>[1911.0, 564.0, 1911.0, 561.0, 1880.0, 561.0, ...</td>\n",
       "      <td>[2, 138]</td>\n",
       "      <td>120</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#000022;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>[1883.0, 578.0, 1911.0, 580.0]</td>\n",
       "      <td>[1911.0, 580.0, 1911.0, 578.0, 1883.0, 578.0, ...</td>\n",
       "      <td>[2, 138]</td>\n",
       "      <td>27</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#000023;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>[1827.0, 561.0, 1857.0, 564.0]</td>\n",
       "      <td>[1857.0, 564.0, 1857.0, 561.0, 1827.0, 561.0, ...</td>\n",
       "      <td>[2, 138]</td>\n",
       "      <td>112</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#000024;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              a_bbox  \\\n",
       "1020   [116.0, 139.0, 2315.0, 206.0]   \n",
       "1021   [116.0, 309.0, 2315.0, 376.0]   \n",
       "1022  [1880.0, 561.0, 1911.0, 564.0]   \n",
       "1023  [1883.0, 578.0, 1911.0, 580.0]   \n",
       "1024  [1827.0, 561.0, 1857.0, 564.0]   \n",
       "\n",
       "                                                 o_bbox      cat_id   area  \\\n",
       "1020  [2315.0, 206.0, 2315.0, 139.0, 116.0, 139.0, 1...  [135, 208]  18945   \n",
       "1021  [2315.0, 376.0, 2315.0, 309.0, 116.0, 309.0, 1...  [135, 208]  19223   \n",
       "1022  [1911.0, 564.0, 1911.0, 561.0, 1880.0, 561.0, ...    [2, 138]    120   \n",
       "1023  [1911.0, 580.0, 1911.0, 578.0, 1883.0, 578.0, ...    [2, 138]     27   \n",
       "1024  [1857.0, 564.0, 1857.0, 561.0, 1827.0, 561.0, ...    [2, 138]    112   \n",
       "\n",
       "     img_id           comments  label  \n",
       "1020    679  instance:#000010;    155  \n",
       "1021    679  instance:#000021;    155  \n",
       "1022    679  instance:#000022;      2  \n",
       "1023    679  instance:#000023;      2  \n",
       "1024    679  instance:#000024;      2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remap the class labels\n",
    "class_mapping = dict(zip(raw_labels['old_id'].astype(str), raw_labels['label']))\n",
    "\n",
    "# Define a function to replace each cat_id list with corresponding class names\n",
    "def map_cat_ids_to_classes(cat_ids):\n",
    "    return [class_mapping.get(str(cat_id)) for cat_id in cat_ids]\n",
    "\n",
    "def clean_labels(label_list):\n",
    "    # Use a set comprehension to remove duplicates and filter out None values\n",
    "    return list({label for label in label_list if label is not None})\n",
    "    \n",
    "def select_highest_precedence(label_list):\n",
    "    return max(label_list)\n",
    "\n",
    "# Apply this function to the cat_id column in train and test obboxs DataFrames\n",
    "train_obboxs['label'] = train_obboxs['cat_id'].apply(map_cat_ids_to_classes)\n",
    "test_obboxs['label'] = test_obboxs['cat_id'].apply(map_cat_ids_to_classes)\n",
    "train_obboxs['label'] = train_obboxs['label'].apply(clean_labels)\n",
    "test_obboxs['label'] = test_obboxs['label'].apply(clean_labels)\n",
    "train_obboxs['label'] = train_obboxs['label'].apply(select_highest_precedence)\n",
    "test_obboxs['label'] = test_obboxs['label'].apply(select_highest_precedence)\n",
    "train_obboxs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7ecf1-218a-4b80-9fe9-ad4d9d01a9e7",
   "metadata": {},
   "source": [
    "#### now we can extract the relative position and duration information. as far as I can tell, this only applies to notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571ea033-d991-4ff1-a0d7-93ebc16d75b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_bbox</th>\n",
       "      <th>o_bbox</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>area</th>\n",
       "      <th>img_id</th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "      <th>rel_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>[1295.0, 134.0, 1296.0, 186.0]</td>\n",
       "      <td>[1296.0, 186.0, 1296.0, 134.0, 1295.0, 134.0, ...</td>\n",
       "      <td>[42, 161]</td>\n",
       "      <td>105</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#000089;</td>\n",
       "      <td>62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>[1321.0, 612.0, 1341.0, 629.0]</td>\n",
       "      <td>[1343.3448486328125, 623.862060546875, 1337.48...</td>\n",
       "      <td>[27, 157]</td>\n",
       "      <td>276</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#00008a;duration:16;rel_position:-13;</td>\n",
       "      <td>45</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>[1340.0, 513.0, 1341.0, 617.0]</td>\n",
       "      <td>[1341.0, 617.0, 1341.0, 513.0, 1340.0, 513.0, ...</td>\n",
       "      <td>[42, 161]</td>\n",
       "      <td>201</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#00008b;</td>\n",
       "      <td>62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>[1358.0, 156.0, 1378.0, 172.0]</td>\n",
       "      <td>[1378.0, 156.0, 1358.0, 156.0, 1358.0, 172.0, ...</td>\n",
       "      <td>[27, 157]</td>\n",
       "      <td>279</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#00008c;duration:8;rel_position:1;</td>\n",
       "      <td>45</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              a_bbox  \\\n",
       "1120  [1295.0, 134.0, 1296.0, 186.0]   \n",
       "1121  [1321.0, 612.0, 1341.0, 629.0]   \n",
       "1122  [1340.0, 513.0, 1341.0, 617.0]   \n",
       "1123  [1358.0, 156.0, 1378.0, 172.0]   \n",
       "\n",
       "                                                 o_bbox     cat_id area  \\\n",
       "1120  [1296.0, 186.0, 1296.0, 134.0, 1295.0, 134.0, ...  [42, 161]  105   \n",
       "1121  [1343.3448486328125, 623.862060546875, 1337.48...  [27, 157]  276   \n",
       "1122  [1341.0, 617.0, 1341.0, 513.0, 1340.0, 513.0, ...  [42, 161]  201   \n",
       "1123  [1378.0, 156.0, 1358.0, 156.0, 1358.0, 172.0, ...  [27, 157]  279   \n",
       "\n",
       "     img_id                                        comments  label  duration  \\\n",
       "1120    679                               instance:#000089;     62       NaN   \n",
       "1121    679  instance:#00008a;duration:16;rel_position:-13;     45      16.0   \n",
       "1122    679                               instance:#00008b;     62       NaN   \n",
       "1123    679     instance:#00008c;duration:8;rel_position:1;     45       8.0   \n",
       "\n",
       "      rel_position  \n",
       "1120           NaN  \n",
       "1121         -13.0  \n",
       "1122           NaN  \n",
       "1123           1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to extract duration and relative position from comments\n",
    "def extract_info(comment):\n",
    "    duration = re.search(r'duration:(\\d+);', comment)\n",
    "    rel_position = re.search(r'rel_position:(-?\\d+);', comment)\n",
    "    return [int(duration.group(1)) if duration else None, int(rel_position.group(1)) if rel_position else None]\n",
    "    \n",
    "# Apply the function to create new columns\n",
    "train_obboxs[['duration', 'rel_position']] = train_obboxs['comments'].apply(extract_info).tolist()\n",
    "test_obboxs[['duration', 'rel_position']] = test_obboxs['comments'].apply(extract_info).tolist()\n",
    "train_obboxs.iloc[100:104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05331b-24d3-49d5-8f99-dca72b120c02",
   "metadata": {},
   "source": [
    "#### We need to get rid of the NAN values for symbols that are not notes. Torch wont allow NAN inputs so we will make two new feature masks which mark the \"true\" values of position/duration and then we can impute the rest with a value unrelated to the true data. Interestingly, rests do not have a duration- we should probably fill these in based on the rest type. We should also calculate the relative position for all of the other symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f222b0bf-65dc-4845-978c-5ec7174949f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_bbox</th>\n",
       "      <th>o_bbox</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>area</th>\n",
       "      <th>img_id</th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "      <th>rel_position</th>\n",
       "      <th>duration_mask</th>\n",
       "      <th>rel_position_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>[1295.0, 134.0, 1296.0, 186.0]</td>\n",
       "      <td>[1296.0, 186.0, 1296.0, 134.0, 1295.0, 134.0, ...</td>\n",
       "      <td>[42, 161]</td>\n",
       "      <td>105</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#000089;</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>[1321.0, 612.0, 1341.0, 629.0]</td>\n",
       "      <td>[1343.3448486328125, 623.862060546875, 1337.48...</td>\n",
       "      <td>[27, 157]</td>\n",
       "      <td>276</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#00008a;duration:16;rel_position:-13;</td>\n",
       "      <td>45</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>[1340.0, 513.0, 1341.0, 617.0]</td>\n",
       "      <td>[1341.0, 617.0, 1341.0, 513.0, 1340.0, 513.0, ...</td>\n",
       "      <td>[42, 161]</td>\n",
       "      <td>201</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#00008b;</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>[1358.0, 156.0, 1378.0, 172.0]</td>\n",
       "      <td>[1378.0, 156.0, 1358.0, 156.0, 1358.0, 172.0, ...</td>\n",
       "      <td>[27, 157]</td>\n",
       "      <td>279</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#00008c;duration:8;rel_position:1;</td>\n",
       "      <td>45</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              a_bbox  \\\n",
       "1120  [1295.0, 134.0, 1296.0, 186.0]   \n",
       "1121  [1321.0, 612.0, 1341.0, 629.0]   \n",
       "1122  [1340.0, 513.0, 1341.0, 617.0]   \n",
       "1123  [1358.0, 156.0, 1378.0, 172.0]   \n",
       "\n",
       "                                                 o_bbox     cat_id area  \\\n",
       "1120  [1296.0, 186.0, 1296.0, 134.0, 1295.0, 134.0, ...  [42, 161]  105   \n",
       "1121  [1343.3448486328125, 623.862060546875, 1337.48...  [27, 157]  276   \n",
       "1122  [1341.0, 617.0, 1341.0, 513.0, 1340.0, 513.0, ...  [42, 161]  201   \n",
       "1123  [1378.0, 156.0, 1358.0, 156.0, 1358.0, 172.0, ...  [27, 157]  279   \n",
       "\n",
       "     img_id                                        comments  label  duration  \\\n",
       "1120    679                               instance:#000089;     62      -1.0   \n",
       "1121    679  instance:#00008a;duration:16;rel_position:-13;     45      16.0   \n",
       "1122    679                               instance:#00008b;     62      -1.0   \n",
       "1123    679     instance:#00008c;duration:8;rel_position:1;     45       8.0   \n",
       "\n",
       "      rel_position  duration_mask  rel_position_mask  \n",
       "1120          50.0              0                  0  \n",
       "1121         -13.0              1                  1  \n",
       "1122          50.0              0                  0  \n",
       "1123           1.0              1                  1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a mask for the duration to mark where the duration is relevent\n",
    "train_obboxs['duration_mask'] = train_obboxs['duration'].notna().astype(int)\n",
    "test_obboxs['duration_mask'] = test_obboxs['duration'].notna().astype(int)\n",
    "# set items with no duration to -1\n",
    "# we may need to reapproach this with another method\n",
    "train_obboxs['duration'] = train_obboxs['duration'].replace(np.nan,-1)\n",
    "test_obboxs['duration'] = test_obboxs['duration'].replace(np.nan,-1)\n",
    "\n",
    "# create a mask for the rel_position to mark where the rel_position is relevent\n",
    "train_obboxs['rel_position_mask'] = train_obboxs['rel_position'].notna().astype(int)\n",
    "test_obboxs['rel_position_mask'] = test_obboxs['rel_position'].notna().astype(int)\n",
    "# set items with no rel_position to 50 (nothing has a position this high)\n",
    "# we may need to reapproach this with a KNN inference\n",
    "train_obboxs['rel_position'] = train_obboxs['rel_position'].replace(np.nan,50)\n",
    "test_obboxs['rel_position'] = test_obboxs['rel_position'].replace(np.nan,50)\n",
    "train_obboxs.iloc[100:104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe473f4-5c06-45bb-b3d1-5096b7c33ae1",
   "metadata": {},
   "source": [
    "#### there are some bounding boxes with 0 width and/or height. We will add 1px padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d52eb1c-b6f7-41e1-9a67-815360731a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_bbox(bbox):\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    if x_min == x_max:\n",
    "        x_min -= 1\n",
    "        x_max += 1\n",
    "    if y_min == y_max:\n",
    "        y_min -= 1\n",
    "        y_max += 1\n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45dc7fe0-3661-4c14-a03c-3bf6a8bb6f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_bbox</th>\n",
       "      <th>o_bbox</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>area</th>\n",
       "      <th>img_id</th>\n",
       "      <th>comments</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "      <th>rel_position</th>\n",
       "      <th>duration_mask</th>\n",
       "      <th>rel_position_mask</th>\n",
       "      <th>padded_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>[116.0, 139.0, 2315.0, 206.0]</td>\n",
       "      <td>[2315.0, 206.0, 2315.0, 139.0, 116.0, 139.0, 1...</td>\n",
       "      <td>[135, 208]</td>\n",
       "      <td>18945</td>\n",
       "      <td>679</td>\n",
       "      <td>instance:#000010;</td>\n",
       "      <td>155</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[116.0, 139.0, 2315.0, 206.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             a_bbox  \\\n",
       "1020  [116.0, 139.0, 2315.0, 206.0]   \n",
       "\n",
       "                                                 o_bbox      cat_id   area  \\\n",
       "1020  [2315.0, 206.0, 2315.0, 139.0, 116.0, 139.0, 1...  [135, 208]  18945   \n",
       "\n",
       "     img_id           comments  label  duration  rel_position  duration_mask  \\\n",
       "1020    679  instance:#000010;    155      -1.0          50.0              0   \n",
       "\n",
       "      rel_position_mask                    padded_bbox  \n",
       "1020                  0  [116.0, 139.0, 2315.0, 206.0]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to the 'a_bbox' column of the DataFrame\n",
    "train_obboxs['padded_bbox'] = train_obboxs['a_bbox'].apply(adjust_bbox)\n",
    "test_obboxs['padded_bbox'] = test_obboxs['a_bbox'].apply(adjust_bbox)\n",
    "train_obboxs['padded_bbox'] = train_obboxs['padded_bbox'].apply(adjust_bbox)\n",
    "test_obboxs['padded_bbox'] = test_obboxs['padded_bbox'].apply(adjust_bbox)\n",
    "train_obboxs.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51b3afd-09ae-4aca-bdf4-e17db2643421",
   "metadata": {},
   "source": [
    "#### clean up the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89b3abd-b980-4e62-b2f8-b6a4576ed68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>a_bbox</th>\n",
       "      <th>o_bbox</th>\n",
       "      <th>area</th>\n",
       "      <th>img_id</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "      <th>rel_position</th>\n",
       "      <th>duration_mask</th>\n",
       "      <th>rel_position_mask</th>\n",
       "      <th>padded_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>[1466.0, 338.0, 1467.0, 413.0]</td>\n",
       "      <td>[1467.0, 413.0, 1467.0, 338.0, 1466.0, 338.0, ...</td>\n",
       "      <td>152</td>\n",
       "      <td>1180</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1466.0, 338.0, 1467.0, 413.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>102</td>\n",
       "      <td>[1500.0, 211.0, 1520.0, 228.0]</td>\n",
       "      <td>[1522.0, 224.00001525878906, 1517.0, 209.00001...</td>\n",
       "      <td>271</td>\n",
       "      <td>1180</td>\n",
       "      <td>45</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1500.0, 211.0, 1520.0, 228.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>103</td>\n",
       "      <td>[1500.0, 318.0, 1520.0, 335.0]</td>\n",
       "      <td>[1523.0, 325.0, 1512.0, 314.0, 1497.5, 328.5, ...</td>\n",
       "      <td>275</td>\n",
       "      <td>1180</td>\n",
       "      <td>43</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[1500.0, 318.0, 1520.0, 335.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>104</td>\n",
       "      <td>[1519.0, 136.0, 1520.0, 217.0]</td>\n",
       "      <td>[1520.0, 217.0, 1520.0, 136.0, 1519.0, 136.0, ...</td>\n",
       "      <td>164</td>\n",
       "      <td>1180</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1519.0, 136.0, 1520.0, 217.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ann_id                          a_bbox  \\\n",
       "100     101  [1466.0, 338.0, 1467.0, 413.0]   \n",
       "101     102  [1500.0, 211.0, 1520.0, 228.0]   \n",
       "102     103  [1500.0, 318.0, 1520.0, 335.0]   \n",
       "103     104  [1519.0, 136.0, 1520.0, 217.0]   \n",
       "\n",
       "                                                o_bbox  area  img_id  label  \\\n",
       "100  [1467.0, 413.0, 1467.0, 338.0, 1466.0, 338.0, ...   152    1180     62   \n",
       "101  [1522.0, 224.00001525878906, 1517.0, 209.00001...   271    1180     45   \n",
       "102  [1523.0, 325.0, 1512.0, 314.0, 1497.5, 328.5, ...   275    1180     43   \n",
       "103  [1520.0, 217.0, 1520.0, 136.0, 1519.0, 136.0, ...   164    1180     62   \n",
       "\n",
       "     duration  rel_position  duration_mask  rel_position_mask  \\\n",
       "100      -1.0          50.0              0                  0   \n",
       "101       8.0          -3.0              1                  1   \n",
       "102       8.0           4.0              1                  1   \n",
       "103      -1.0          50.0              0                  0   \n",
       "\n",
       "                        padded_bbox  \n",
       "100  [1466.0, 338.0, 1467.0, 413.0]  \n",
       "101  [1500.0, 211.0, 1520.0, 228.0]  \n",
       "102  [1500.0, 318.0, 1520.0, 335.0]  \n",
       "103  [1519.0, 136.0, 1520.0, 217.0]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean up\n",
    "train_obboxs.reset_index(inplace=True)\n",
    "test_obboxs.reset_index(inplace=True)\n",
    "train_obboxs.drop(['cat_id','comments'], axis=1, inplace=True)\n",
    "test_obboxs.drop(['cat_id','comments'], axis=1, inplace=True)\n",
    "train_obboxs.rename(columns={'index': 'ann_id'}, inplace=True)\n",
    "test_obboxs.rename(columns={'index': 'ann_id'}, inplace=True)\n",
    "train_obboxs['ann_id'] = train_obboxs['ann_id'].astype(int)\n",
    "test_obboxs['ann_id'] = test_obboxs['ann_id'].astype(int)\n",
    "train_obboxs['area'] = train_obboxs['area'].astype(int)\n",
    "test_obboxs['area'] = test_obboxs['area'].astype(int)\n",
    "train_obboxs['img_id'] = train_obboxs['img_id'].astype(int)\n",
    "test_obboxs['img_id'] = test_obboxs['img_id'].astype(int)\n",
    "test_obboxs.iloc[100:104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb5e82f-5bb7-4c29-ab8b-4ef432f79e28",
   "metadata": {},
   "source": [
    "## join the tables together to get one big table with the complete info for every annotation-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2521e38d-4dc1-4172-ada5-7a27aa292d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_id</th>\n",
       "      <th>a_bbox</th>\n",
       "      <th>o_bbox</th>\n",
       "      <th>area</th>\n",
       "      <th>img_id</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "      <th>rel_position</th>\n",
       "      <th>duration_mask</th>\n",
       "      <th>rel_position_mask</th>\n",
       "      <th>padded_bbox</th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1020</td>\n",
       "      <td>[116.0, 139.0, 2315.0, 206.0]</td>\n",
       "      <td>[2315.0, 206.0, 2315.0, 139.0, 116.0, 139.0, 1...</td>\n",
       "      <td>18945</td>\n",
       "      <td>679</td>\n",
       "      <td>155</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[116.0, 139.0, 2315.0, 206.0]</td>\n",
       "      <td>lg-877777775968732096-aug-gonville--page-3.png</td>\n",
       "      <td>2431</td>\n",
       "      <td>3439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ann_id                         a_bbox  \\\n",
       "0    1020  [116.0, 139.0, 2315.0, 206.0]   \n",
       "\n",
       "                                              o_bbox   area  img_id  label  \\\n",
       "0  [2315.0, 206.0, 2315.0, 139.0, 116.0, 139.0, 1...  18945     679    155   \n",
       "\n",
       "   duration  rel_position  duration_mask  rel_position_mask  \\\n",
       "0      -1.0          50.0              0                  0   \n",
       "\n",
       "                     padded_bbox  \\\n",
       "0  [116.0, 139.0, 2315.0, 206.0]   \n",
       "\n",
       "                                         filename  width  height  \n",
       "0  lg-877777775968732096-aug-gonville--page-3.png   2431    3439  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.merge(train_obboxs, train_images, on='img_id', how='inner')\n",
    "test_data = pd.merge(test_obboxs, test_images, on='img_id', how='inner')\n",
    "train_data.drop('ann_ids', axis=1, inplace=True)\n",
    "test_data.drop('ann_ids', axis=1, inplace=True)\n",
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49631ae",
   "metadata": {},
   "source": [
    "## concatenate the barline annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dac9404",
   "metadata": {},
   "outputs": [],
   "source": [
    "barlines_df = pd.read_csv('./data/ds2_dense/barlines.csv')\n",
    "\n",
    "def convert_str_to_list(coord_str):\n",
    "    return ast.literal_eval(coord_str)\n",
    "\n",
    "barlines_df['a_bbox'] = barlines_df['a_bbox'].apply(convert_str_to_list)\n",
    "barlines_df['o_bbox'] = barlines_df['o_bbox'].apply(convert_str_to_list)\n",
    "barlines_df['padded_bbox'] = barlines_df['padded_bbox'].apply(convert_str_to_list)\n",
    "\n",
    "missing_annotations = barlines_df[barlines_df['filename'].isin(train_data['filename'])]\n",
    "train_data = pd.concat([train_data, missing_annotations], ignore_index=True)\n",
    "\n",
    "missing_annotations = barlines_df[barlines_df['filename'].isin(test_data['filename'])]\n",
    "test_data = pd.concat([test_data, missing_annotations], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b35dd-e480-48aa-9f00-e2b97d0d2608",
   "metadata": {},
   "source": [
    "### compute the yolo bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d941f-8997-4a8b-aed4-1e7dc590ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corners_to_yolo(bbox, img_width, img_height):\n",
    "    polygon = Polygon([(bbox[i], bbox[i + 1]) for i in range(0, len(bbox), 2)])\n",
    "    min_rect = polygon.minimum_rotated_rectangle\n",
    "\n",
    "    # Check if the minimum rotated rectangle is a point\n",
    "    if isinstance(min_rect, Point):\n",
    "        # Handle the case where the shape is a point by creating a small box around it\n",
    "        x, y = min_rect.x, min_rect.y\n",
    "        min_rect = Polygon([(x-1, y-1), (x+1, y-1), (x+1, y+1), (x-1, y+1)])\n",
    "        return 'invalid'\n",
    "\n",
    "    # check if symbol is a line and add 1 px padding if so (almost always stems)\n",
    "    elif isinstance(min_rect, LineString):\n",
    "        # Handle the case where the shape is a line by padding\n",
    "        x_coords, y_coords = zip(*min_rect.coords)\n",
    "        min_x, max_x = min(x_coords), max(x_coords)\n",
    "        min_y, max_y = min(y_coords), max(y_coords)\n",
    "        min_rect = Polygon([(min_x-1, min_y-1), (max_x+1, min_y-1), (max_x+1, max_y+1), (min_x-1, max_y+1)])\n",
    "\n",
    "    corners = np.array(min_rect.exterior.coords)\n",
    "    edge1 = np.linalg.norm(corners[0] - corners[1])\n",
    "    edge2 = np.linalg.norm(corners[1] - corners[2])\n",
    "    width = max(edge1, edge2)\n",
    "    height = min(edge1, edge2)\n",
    "    center = min_rect.centroid.coords[0]\n",
    "    center_x = center[0]\n",
    "    center_y = center[1]\n",
    "    angle = np.rad2deg(np.arctan2(corners[1][1] - corners[0][1], corners[1][0] - corners[0][0]))\n",
    "\n",
    "    center_x /= img_width\n",
    "    center_y /= img_height\n",
    "    width /= img_width\n",
    "    height /= img_height\n",
    "\n",
    "    return [center_x, center_y, width, height, angle]\n",
    "    \n",
    "# Function to convert corners to YOLO format for each row in the DataFrame\n",
    "def apply_corners_to_yolo(row):\n",
    "    return corners_to_yolo(row['o_bbox'], row['width'], row['height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35fd744-0634-4b48-b2b9-7c2d44c4cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with bounding boxes in (center x, center y, W, H, R)*normalized format\n",
    "train_data['yolo_bbox'] = train_data.apply(apply_corners_to_yolo, axis=1)\n",
    "test_data['yolo_bbox'] = test_data.apply(apply_corners_to_yolo, axis=1)\n",
    "# drop invalid boxes\n",
    "train_data = train_data[train_data['yolo_bbox']!='invalid']\n",
    "test_data = test_data[test_data['yolo_bbox']!='invalid']\n",
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded23c5-0c7a-445f-8131-6426cc4aa0ff",
   "metadata": {},
   "source": [
    "## aggregate the data so there is one image per line. all other variables will be aggregated into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd4716b1-e04d-4134-867b-d33d4964d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_agg = train_data.groupby('filename').agg({\n",
    "    'ann_id': lambda x: list(x),\n",
    "    'a_bbox': lambda x: list(x),\n",
    "    'o_bbox': lambda x: list(x),\n",
    "    'padded_bbox': lambda x: list(x),\n",
    "    'area': lambda x: list(x),\n",
    "    'duration': lambda x: list(x),\n",
    "    'duration_mask': lambda x: list(x),\n",
    "    'rel_position': lambda x: list(x), \n",
    "    'rel_position_mask': lambda x: list(x),\n",
    "    'label': lambda x: list(x),\n",
    "    'img_id': 'first',  # assuming all entries per image have the same img_id\n",
    "    'width': 'first',   # assuming all entries per image have the same width\n",
    "    'height': 'first'  # assuming all entries per image have the same height\n",
    "}).reset_index()\n",
    "test_data_agg = test_data.groupby('filename').agg({\n",
    "    'ann_id': lambda x: list(x),\n",
    "    'a_bbox': lambda x: list(x),\n",
    "    'o_bbox': lambda x: list(x),\n",
    "    'padded_bbox': lambda x: list(x),\n",
    "    'area': lambda x: list(x),\n",
    "    'duration': lambda x: list(x),\n",
    "    'duration_mask': lambda x: list(x),\n",
    "    'rel_position': lambda x: list(x), \n",
    "    'rel_position_mask': lambda x: list(x),\n",
    "    'label': lambda x: list(x),\n",
    "    'img_id': 'first',  # assuming all entries per image have the same img_id\n",
    "    'width': 'first',   # assuming all entries per image have the same width\n",
    "    'height': 'first'  # assuming all entries per image have the same height\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c4a66e-096d-44d0-8841-106da6c3169a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>ann_id</th>\n",
       "      <th>a_bbox</th>\n",
       "      <th>o_bbox</th>\n",
       "      <th>padded_bbox</th>\n",
       "      <th>area</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration_mask</th>\n",
       "      <th>rel_position</th>\n",
       "      <th>rel_position_mask</th>\n",
       "      <th>label</th>\n",
       "      <th>img_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lg-101766503886095953-aug-beethoven--page-1.png</td>\n",
       "      <td>[632316, 632317, 632318, 632319, 632320, 63232...</td>\n",
       "      <td>[[233.0, 376.0, 1866.0, 443.0], [233.0, 833.0,...</td>\n",
       "      <td>[[1866.0, 443.0, 1866.0, 376.0, 233.0, 376.0, ...</td>\n",
       "      <td>[[233.0, 376.0, 1866.0, 443.0], [233.0, 833.0,...</td>\n",
       "      <td>[13425, 9986, 73, 75, 12613, 10958, 105, 269, ...</td>\n",
       "      <td>[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 8.0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, ...</td>\n",
       "      <td>[50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, -2....</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, ...</td>\n",
       "      <td>[155, 155, 2, 2, 155, 155, 62, 43, 112, 45, 62...</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1960</td>\n",
       "      <td>2772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          filename  \\\n",
       "0  lg-101766503886095953-aug-beethoven--page-1.png   \n",
       "\n",
       "                                              ann_id  \\\n",
       "0  [632316, 632317, 632318, 632319, 632320, 63232...   \n",
       "\n",
       "                                              a_bbox  \\\n",
       "0  [[233.0, 376.0, 1866.0, 443.0], [233.0, 833.0,...   \n",
       "\n",
       "                                              o_bbox  \\\n",
       "0  [[1866.0, 443.0, 1866.0, 376.0, 233.0, 376.0, ...   \n",
       "\n",
       "                                         padded_bbox  \\\n",
       "0  [[233.0, 376.0, 1866.0, 443.0], [233.0, 833.0,...   \n",
       "\n",
       "                                                area  \\\n",
       "0  [13425, 9986, 73, 75, 12613, 10958, 105, 269, ...   \n",
       "\n",
       "                                            duration  \\\n",
       "0  [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 8.0...   \n",
       "\n",
       "                                       duration_mask  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, ...   \n",
       "\n",
       "                                        rel_position  \\\n",
       "0  [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, -2....   \n",
       "\n",
       "                                   rel_position_mask  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, ...   \n",
       "\n",
       "                                               label  img_id  width  height  \n",
       "0  [155, 155, 2, 2, 155, 155, 62, 43, 112, 45, 62...   142.0   1960    2772  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_agg.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dfe7b6-bb08-4d7b-b6bc-45a60f229f45",
   "metadata": {},
   "source": [
    "### the last thing is to handle the differing image sizes. The problem here is that if you add padding, or resieze the image, you have to go back and adjust the coordinates of all of the annotations for that image. For now, we will just select the most common image size which represents the majority of our images, and use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8feed9e2-689b-4814-a296-2b1a7bc35280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3842, 5434)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_width = max(train_data['width'].max(), test_data['width'].max())\n",
    "max_height = max(train_data['height'].max(), test_data['height'].max())\n",
    "max_width, max_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a91b402-fbd6-4032-bb2f-5a943ef816f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1960.0, 2772.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_agg['width'].median(), train_data_agg['height'].median() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68ae89e3-3b75-4694-8e24-19c8480a452b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of images that are the median width/height:\n",
      "0.7540381791483113\n",
      "0.7215909090909091\n"
     ]
    }
   ],
   "source": [
    "train_data_agg_final = train_data_agg[(train_data_agg['width'] == 1960) & (train_data_agg['height'] == 2772)]\n",
    "test_data_agg_final = test_data_agg[(test_data_agg['width'] == 1960) & (test_data_agg['height'] == 2772)]\n",
    "print(\"Proportion of images that are the median width/height:\")\n",
    "print(len(train_data_agg_final)/len(train_data_agg))        \n",
    "print(len(test_data_agg_final)/len(test_data_agg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e81a6b5-27af-44e4-9646-5805bfcb83e1",
   "metadata": {},
   "source": [
    "## Prepare the Torch dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c39ab1a4-ad58-4428-b4dd-2d3bd4dd7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicScoreDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (DataFrame): Pandas DataFrame containing annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transforms (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.annotations = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        # Set default transforms if none are provided\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),  # Convert images to tensors\n",
    "                # transforms.Normalize(mean=[0.485], std=[0.229])  # Adjust if your image is not RGB\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.annotations['filename'].iloc[idx])\n",
    "        image = Image.open(img_name).convert(\"L\") # use grayscale\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        boxes = torch.as_tensor(self.annotations['padded_bbox'].iloc[idx], dtype=torch.float32)\n",
    "        durations = torch.as_tensor(self.annotations['duration'].iloc[idx], dtype=torch.float32)\n",
    "        rel_positions = torch.as_tensor(self.annotations['rel_position'].iloc[idx], dtype=torch.float32)\n",
    "        duration_masks = torch.as_tensor(self.annotations['duration_mask'].iloc[idx], dtype=torch.int32)\n",
    "        rel_position_masks = torch.as_tensor(self.annotations['rel_position_mask'].iloc[idx], dtype=torch.int32)\n",
    "        labels = torch.as_tensor(self.annotations['label'].iloc[idx], dtype=torch.int64)\n",
    "        image_id = torch.tensor([self.annotations['img_id'].iloc[idx]], dtype=torch.int64)\n",
    "        area = torch.as_tensor([self.annotations['area'].iloc[idx]], dtype=torch.float32)\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)  # Assuming no crowd\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"durations\"] = durations\n",
    "        target[\"rel_positions\"] = rel_positions\n",
    "        target[\"duration_masks\"] = duration_masks\n",
    "        target[\"rel_position_masks\"] = rel_position_masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2fec48e-587a-4bf5-af96-7c6d74aaf575",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MusicScoreDataset(train_data_agg_final, './data/ds2_dense/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eef7b9b0-6a41-46d4-9536-d020c5ac9246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " {'boxes': tensor([[ 233.,  376., 1866.,  443.],\n",
       "          [ 233.,  833., 1866.,  899.],\n",
       "          [1633.,  587., 1666.,  589.],\n",
       "          ...,\n",
       "          [ 732., 2267.,  752., 2284.],\n",
       "          [ 751., 2187.,  752., 2272.],\n",
       "          [ 752., 2188.,  769., 2234.]]),\n",
       "  'labels': tensor([155, 155,   2,   2, 155, 155,  62,  43, 112,  45,  62,  50,  48,  43,\n",
       "           62,  62,  62,  43,  45,  45,  43,  69,  62,  62, 139,  62,  62,  43,\n",
       "           45,  45,  45,  69,  62, 112,  45,  62,  45,  43,  43,  45,  62, 112,\n",
       "           45,  69,  62, 139,  69,  62,  48,  45,  69,  62,  45,  43,  43, 112,\n",
       "          112, 111,  69,  62,  62,  45,  43,  43,  43,  62,  45,  62,  62,  50,\n",
       "          112, 111,  69,  62,  43,  43,  43,  50,  69,  62,  43,  43,  43,  69,\n",
       "           62,  43,  43,  62,  62,  50, 112,  91,  91,  91,  91,   6, 112,  28,\n",
       "           28,  91,  91,  91,  91,  10,  50,  28,  28,  91,  91,  91,  91,   6,\n",
       "          110,  28,  28,   1,  62,  43,  62,  28,  28,  91,  91,  91,  91,  10,\n",
       "           50,  69,  62,  43,  45,  45,  62,  45,  62,  48, 112,  50,  45,  69,\n",
       "           62,  43,  45, 112, 111,  62,  48, 112,  50,  43,  69,  62,  43,  43,\n",
       "           43, 112,  45,  62,  62, 155,   2,   2, 155, 155,   2,   2,   2, 155,\n",
       "           62,  45,  45,  45,  62,  62,  50, 112,  50,  43, 112,  45,  45,  43,\n",
       "           62,  62,  43,  43,  69,  62,  45,  62,  62,  62, 139,  45,  45,  45,\n",
       "           43,  62,  62,  69,  62, 112,  43,  69,  62,  43,  45,  45,  43,  62,\n",
       "           43,  45,  62,  69, 112,  62,  62,  48, 112,  48,  50,  69, 139,  62,\n",
       "           69,  62,  45,  43,  43,  62,  62,  62,  48, 112,  48,  45,  69,  69,\n",
       "           45,  43,  43,  62, 112,  45,  62,  62,  45,  43,  43,  62, 111,  48,\n",
       "          112,  48, 112,  62,  43,  45,  45,  62,  69,  45,  43,  43,  45,  75,\n",
       "           62, 111, 112, 112,  69,  62,  45,  45,  43,   6,   6,  91,  91,  91,\n",
       "           91,  10,  91,  91,  91,  91,  91,  91,  91,  91,  10,  91,  91,  91,\n",
       "           91,   1,  43,  62,  45,  45,  62,  69,  62,  62,  62,  48, 112,  48,\n",
       "           45,  50,  50, 112,  50,  62,  69,  62,  62,  43,  43,  43,  69,  62,\n",
       "          112,  45,  50, 112,  50, 111,  69,  62,  43,  43,  43, 112,  69,  62,\n",
       "           43,  43,  43,  62,  62,  62,  62,   4,  50, 112,  50,  45,  43,  43,\n",
       "           43,  62,  69]),\n",
       "  'durations': tensor([-1., -1., -1., -1., -1., -1., -1.,  8., -1.,  4., -1.,  2.,  2.,  8.,\n",
       "          -1., -1., -1.,  8.,  8.,  8.,  8., -1., -1., -1., -1., -1., -1.,  8.,\n",
       "           8.,  8.,  8., -1., -1., -1.,  8., -1.,  8.,  8.,  8.,  8., -1., -1.,\n",
       "           8., -1., -1., -1., -1., -1.,  2.,  8., -1., -1.,  8.,  8.,  8., -1.,\n",
       "          -1., -1., -1., -1., -1.,  8.,  8.,  8.,  8., -1.,  8., -1., -1.,  2.,\n",
       "          -1., -1., -1., -1.,  8.,  8.,  8.,  2., -1., -1.,  8.,  8.,  8., -1.,\n",
       "          -1.,  8.,  8., -1., -1.,  2., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1.,  2., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1.,  8., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "           2., -1., -1.,  8.,  8.,  8., -1.,  8., -1.,  2., -1.,  2.,  8., -1.,\n",
       "          -1.,  8.,  8., -1., -1., -1.,  2., -1.,  2.,  4., -1., -1.,  8.,  8.,\n",
       "           8., -1.,  4., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1.,  8.,  8.,  8., -1., -1.,  2., -1.,  2.,  8., -1.,  8.,  8.,  8.,\n",
       "          -1., -1.,  8.,  8., -1., -1.,  8., -1., -1., -1., -1.,  8.,  8.,  8.,\n",
       "           8., -1., -1., -1., -1., -1.,  8., -1., -1.,  8.,  8.,  8.,  8., -1.,\n",
       "           8.,  8., -1., -1., -1., -1., -1.,  2., -1.,  2.,  2., -1., -1., -1.,\n",
       "          -1., -1.,  8.,  8.,  8., -1., -1., -1.,  2., -1.,  2.,  4., -1., -1.,\n",
       "           8.,  8.,  8., -1., -1.,  4., -1., -1.,  8.,  8.,  8., -1., -1.,  2.,\n",
       "          -1.,  2., -1., -1.,  8.,  8.,  8., -1., -1.,  8.,  8.,  8.,  8., -1.,\n",
       "          -1., -1., -1., -1., -1., -1.,  8.,  8.,  8., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1.,  8., -1.,  8.,  8., -1., -1., -1., -1., -1.,  2., -1.,  2.,\n",
       "           4.,  2.,  2., -1.,  2., -1., -1., -1., -1.,  8.,  8.,  8., -1., -1.,\n",
       "          -1.,  4.,  2., -1.,  2., -1., -1., -1.,  8.,  8.,  8., -1., -1., -1.,\n",
       "           8.,  8.,  8., -1., -1., -1., -1., -1.,  2., -1.,  2.,  4.,  8.,  8.,\n",
       "           8., -1., -1.]),\n",
       "  'rel_positions': tensor([50., 50., 50., 50., 50., 50., 50., -2., 50.,  1., 50., -1.,  6., -4.,\n",
       "          50., 50., 50.,  0., -3., -5.,  4., 50., 50., 50., 50., 50., 50.,  0.,\n",
       "          -3., -5.,  3., 50., 50., 50.,  5., 50.,  1., -2., -4.,  3., 50., 50.,\n",
       "           5., 50., 50., 50., 50., 50.,  6.,  3., 50., 50.,  1., -2., -4., 50.,\n",
       "          50., 50., 50., 50., 50.,  1., -2., -4.,  4., 50.,  1., 50., 50., -1.,\n",
       "          50., 50., 50., 50.,  0., -2., -4.,  1., 50., 50.,  0., -2., -4., 50.,\n",
       "          50.,  0., -2., 50., 50.,  3., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "          50., 50., 50., 50., 50., 50.,  1., 50., 50., 50., 50., 50., 50., 50.,\n",
       "          50., 50., 50., 50., 50., -4., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "           3., 50., 50.,  0., -3., -5., 50., -3., 50., -2., 50.,  5.,  3., 50.,\n",
       "          50.,  0., -5., 50., 50., 50.,  2., 50.,  5.,  4., 50., 50.,  0., -2.,\n",
       "          -4., 50.,  5., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "          50.,  5., -5., -3., 50., 50.,  5., 50.,  5.,  6., 50., -5., -3.,  0.,\n",
       "          50., 50.,  0.,  4., 50., 50.,  3., 50., 50., 50., 50.,  5., -5., -3.,\n",
       "           0., 50., 50., 50., 50., 50.,  4., 50., 50.,  0., -3., -5., -4., 50.,\n",
       "          -2.,  1., 50., 50., 50., 50., 50.,  6., 50.,  6.,  5., 50., 50., 50.,\n",
       "          50., 50.,  1., -2., -4., 50., 50., 50.,  6., 50.,  6.,  5., 50., 50.,\n",
       "           1., -2., -4., 50., 50.,  5., 50., 50.,  1., -2., -4., 50., 50.,  4.,\n",
       "          50.,  4., 50., 50., -4., -1.,  1., 50., 50.,  1.,  0., -4., -1., 50.,\n",
       "          50., 50., 50., 50., 50., 50.,  1., -1., -4., 50., 50., 50., 50., 50.,\n",
       "          50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
       "          50., 50., -4., 50., -1.,  1., 50., 50., 50., 50., 50.,  4., 50.,  4.,\n",
       "           1.,  5.,  1., 50.,  1., 50., 50., 50., 50.,  0., -2., -4., 50., 50.,\n",
       "          50.,  5.,  5., 50.,  5., 50., 50., 50.,  0., -2., -4., 50., 50., 50.,\n",
       "           0., -2., -4., 50., 50., 50., 50., 50.,  1., 50.,  1.,  5., -2.,  0.,\n",
       "          -4., 50., 50.]),\n",
       "  'duration_masks': tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "          0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "          0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "          0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "          0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "          1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "          1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0], dtype=torch.int32),\n",
       "  'rel_position_masks': tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "          1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "          0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "          0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "          0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "          0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "          1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "          1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0], dtype=torch.int32),\n",
       "  'image_id': tensor([142]),\n",
       "  'area': tensor([[13425.,  9986.,    73.,    75., 12613., 10958.,   105.,   269.,   198.,\n",
       "             281.,   112.,   197.,   209.,   282.,   165.,   141.,    87.,   290.,\n",
       "             282.,   288.,   289.,   143.,   104.,   106.,   987.,   132.,    78.,\n",
       "             292.,   285.,   285.,   290.,   147.,   109.,   203.,   290.,    72.,\n",
       "             273.,   269.,   282.,   287.,   103.,   196.,   286.,   125.,   228.,\n",
       "             993.,   126.,   184.,   204.,   284.,   128.,   148.,   290.,   284.,\n",
       "             288.,   201.,   198.,   326.,   129.,   170.,    93.,   285.,   284.,\n",
       "             283.,   285.,   104.,   288.,   110.,    84.,   210.,   197.,   315.,\n",
       "             127.,   147.,   279.,   286.,   283.,   202.,   126.,   209.,   279.,\n",
       "             285.,   283.,   125.,   146.,   280.,   285.,   102.,   110.,   208.,\n",
       "             198.,   426.,   426.,   428.,   426.,  1635.,   197.,   300.,   290.,\n",
       "             426.,   428.,   428.,   426.,   651.,   201.,   286.,   295.,   429.,\n",
       "             426.,   426.,   428.,  1638.,   189.,   314.,   286.,  1280.,   101.,\n",
       "             284.,   110.,   300.,   290.,   426.,   428.,   428.,   427.,   652.,\n",
       "             207.,   132.,   149.,   290.,   284.,   285.,   105.,   267.,    78.,\n",
       "             209.,   202.,   208.,   290.,   133.,   165.,   288.,   285.,   203.,\n",
       "             317.,   104.,   214.,   202.,   204.,   289.,   125.,   145.,   279.,\n",
       "             286.,   285.,   198.,   279.,   112.,   110., 11575.,    97.,    97.,\n",
       "           15505., 13765.,    49.,    97.,    97., 15504.,   103.,   280.,   277.,\n",
       "             274.,   103.,    86.,   207.,   195.,   207.,   287.,   202.,   274.,\n",
       "             271.,   269.,    72.,    77.,   279.,   287.,   123.,   190.,   285.,\n",
       "              88.,   103.,   103.,   294.,   283.,   275.,   274.,   273.,   104.,\n",
       "             174.,   146.,    87.,   200.,   292.,   120.,   242.,   286.,   278.,\n",
       "             285.,   277.,   103.,   270.,   272.,   167.,   151.,   196.,   103.,\n",
       "             103.,   210.,   196.,   210.,   209.,   136.,   935.,   147.,   135.,\n",
       "             143.,   284.,   280.,   286.,   157.,   157.,   157.,   210.,   199.,\n",
       "             207.,   286.,   133.,   129.,   290.,   283.,   284.,   103.,   196.,\n",
       "             284.,   190.,   151.,   287.,   286.,   289.,   104.,   315.,   209.,\n",
       "             196.,   204.,   196.,   110.,   279.,   268.,   274.,   167.,   150.,\n",
       "             263.,   275.,   279.,   270.,   151.,   167.,   314.,   195.,   200.,\n",
       "             136.,   146.,   284.,   285.,   285.,  1640.,  1633.,   426.,   426.,\n",
       "             429.,   429.,   653.,   431.,   426.,   426.,   428.,   429.,   428.,\n",
       "             426.,   426.,   651.,   429.,   426.,   426.,   428.,  1285.,   284.,\n",
       "             158.,   267.,   273.,   167.,   145.,   159.,   158.,   155.,   208.,\n",
       "             202.,   208.,   287.,   202.,   203.,   199.,   205.,   110.,   135.,\n",
       "             110.,   132.,   278.,   281.,   286.,   151.,   103.,   195.,   290.,\n",
       "             209.,   196.,   208.,   318.,   123.,   146.,   283.,   280.,   291.,\n",
       "             202.,   124.,   213.,   281.,   280.,   287.,   110.,   103.,   103.,\n",
       "             103.,  1188.,   208.,   197.,   208.,   285.,   262.,   269.,   279.,\n",
       "             156.,   149.]]),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d71b0-2cf5-4543-9044-6a10d09dbbcc",
   "metadata": {},
   "source": [
    "## prepare the model- we will use faster r-cnn resnet50 and define a collate function to handle the changin size of the inputs (since each image will have a different number of annotations, yet Torch expects a fixed size tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a18062b-ebce-46da-8b2f-5396d73fac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    # Load a model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Replace the classifier with a new one, that has\n",
    "    # num_classes which is user-defined\n",
    "    num_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(num_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e423823-b79e-40ca-9d23-cfcadd0d96e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]  # Extract images\n",
    "    batch_targets = [item[1] for item in batch]  # Extract targets\n",
    "\n",
    "    # Stack images using torch.stack to create a batch\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Initialize the structure for collated targets\n",
    "    # We need a list of dictionaries\n",
    "    collated_targets = []\n",
    "    keys = batch_targets[0].keys()\n",
    "\n",
    "    # We iterate through each batch item to separate their components properly\n",
    "    for index in range(len(batch)):  # Loop over items in the batch\n",
    "        single_target = {}\n",
    "        for key in keys:\n",
    "            # We extract the component for each target key from each batch individually\n",
    "            # This avoids any mixing of data across the batch items\n",
    "            single_target[key] = batch_targets[index][key]\n",
    "        collated_targets.append(single_target)\n",
    "\n",
    "    return images, collated_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82095ce-0d98-43cd-b400-76fc030056f4",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "287bd6d5-8e4d-4855-8b97-51889963add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, targets in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            # Ensure targets are dictionaries and move them to the appropriate device\n",
    "            targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward and backward passes\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1} of {num_epochs}, Loss: {losses.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c192bf08-479c-4377-872c-9b35223d747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = len(unique_labels) + 1  # Define the number of classes including background\n",
    "model = get_model(num_classes).to(device)\n",
    "data_loader = DataLoader(MusicScoreDataset(train_data_agg_final, './data/ds2_dense/images/'), batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "optimizer = SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e345793-7394-485d-aea3-bc3ed5401080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "train_model(model, data_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "949091cf-f105-4866-9fcd-ec7342131c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './exported_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28aa9b-f818-4596-9277-02480c1de250",
   "metadata": {},
   "source": [
    "### the model runs! let's test it - you can grab the model from the google drive if you dont want to spend hours training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b284cdb9-3b82-45a5-ae87-36e66c99ba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/daniel/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/daniel/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|| 97.8M/97.8M [00:08<00:00, 12.7MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=156, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=624, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the backbone\n",
    "backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
    "# Create the model\n",
    "model = FasterRCNN(backbone, num_classes=num_classes)  # num_classes includes the background\n",
    "model.load_state_dict(torch.load('./exported_model.pt'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60789b8a-55b8-4ceb-8ef1-13972cd6b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to a dictionary\n",
    "label_dict = dict(zip(unique_labels['label'], unique_labels['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de5b2fae-f047-4d7d-a976-8c3faf38a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image_path = './data/ds2_dense/images/lg-101766503886095953-aug-beethoven--page-1.png'\n",
    "image = Image.open(image_path).convert(\"L\")\n",
    "image_tensor = F.to_tensor(image).unsqueeze(0)  # Convert image to tensor\n",
    "\n",
    "# Perform prediction\n",
    "with torch.no_grad():\n",
    "    predictions = model(image_tensor)\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "# Draw predictions on the image\n",
    "draw = ImageDraw.Draw(image)\n",
    "# Specify a larger font size for the annotations\n",
    "font = ImageFont.load_default()  # Currently, load_default does not support size adjustment in PIL\n",
    "\n",
    "for element in range(len(predictions[0]['boxes'])):\n",
    "    boxes = predictions[0]['boxes'][element].cpu().numpy().astype(int)\n",
    "    label_id = predictions[0]['labels'][element].item()\n",
    "    score = predictions[0]['scores'][element].item()\n",
    "\n",
    "    # Look up the label name using the label dictionary\n",
    "    label_name = label_dict.get(label_id, 'Unknown')  # Default to 'Unknown' if not found\n",
    "\n",
    "    if score > 0.5:  # filter out low-confidence predictions\n",
    "        draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])], outline='red', width=3)\n",
    "        draw.text((boxes[0], boxes[1]-40), f'{label_name}:{score:.2f}', fill='blue', font=font)  \n",
    "# Save or show image\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51471a95-097a-47c6-bb92-c661ef38763a",
   "metadata": {},
   "source": [
    "## adding noise to an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d90f5d-52e4-440f-8957-05c635a4a1db",
   "metadata": {},
   "source": [
    "#### helper function to draw image with bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b024796-3649-47c8-825e-9976279b7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_with_boxes(image_path_or_bin, bounding_boxes):\n",
    "    \"\"\"\n",
    "    Displays an image with bounding boxes drawn over it. Can handle both orthogonal and oriented bounding boxes.\n",
    "\n",
    "    :param image_path: The path to the image file.\n",
    "    :param bounding_boxes: A list of tuples representing the bounding boxes.\n",
    "                           For orthogonal boxes: (x1, y1, x2, y2)\n",
    "                           For oriented boxes: (x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "    :param image_binary: An alternative to image_path, a PIL image object to be used directly.\n",
    "    :param oriented: Boolean flag to indicate whether the bounding boxes are oriented.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    if type(image_path_or_bin) is str:\n",
    "        image = Image.open(image_path_or_bin)\n",
    "    else:\n",
    "        image = image_path_or_bin\n",
    "\n",
    "    image = image.convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    if len(bounding_boxes[0])==8:\n",
    "        # Draw each bounding box\n",
    "        for bbox in bounding_boxes:\n",
    "            # Assumes oriented bounding box is given as (x1, y1, x2, y2, x3, y3, x4, y4)\n",
    "            # We need to reorganize this into [(x1, y1), (x2, y2), (x3, y3), (x4, y4)]\n",
    "            points = [(bbox[i], bbox[i+1]) for i in range(0, len(bbox), 2)]\n",
    "            draw.polygon(points, outline='red', width=2)\n",
    "    else: \n",
    "        # Draw each bounding box\n",
    "        for bbox in bounding_boxes:\n",
    "            # Orthogonal bounding box (x1, y1, x2, y2)\n",
    "            draw.rectangle(bbox, outline='red', width=2)\n",
    "\n",
    "    # Show the image\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eefcb125-5ad6-4a07-9ae4-cb85f9a3b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './data/ds2_dense/images/lg-102414375-aug-beethoven--page-2.png'\n",
    "image_bin = Image.open(image_path)\n",
    "bboxes = train_data_agg_final[train_data_agg_final['filename']=='lg-102414375-aug-beethoven--page-2.png']['a_bbox']\n",
    "obboxes = train_data_agg_final[train_data_agg_final['filename']=='lg-102414375-aug-beethoven--page-2.png']['o_bbox']\n",
    "bboxes = bboxes.values[0]\n",
    "obboxes = obboxes.values[0]\n",
    "show_image_with_boxes(image_bin, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb942964-ac38-4e8f-a4cc-2f95511063af",
   "metadata": {},
   "source": [
    "### blur - make about 25% of the image blurry (varying intensity randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "12bcd2e2-7a1b-4a94-8eb5-b2db50efa372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_blur(image_path_or_bin, blur_radius=None):\n",
    "    \n",
    "    # Load the image\n",
    "    if type(image_path_or_bin) is str:\n",
    "        image = Image.open(image_path_or_bin)\n",
    "    else:\n",
    "        image = image_path_or_bin\n",
    "    \n",
    "    # set the intensity of the blur\n",
    "    if blur_radius is None:\n",
    "        blur_radius = random.randint(1, 3)\n",
    "        \n",
    "    # Define the area to blur (x1, y1, x2, y2)\n",
    "    x1 = random.randint(0, image.width*0.5)\n",
    "    y1 = random.randint(0, image.width*0.5)\n",
    "    x2 = x1 + int(image.width*0.5) \n",
    "    y2 = y1 + int(image.width*0.5) \n",
    "\n",
    "    # Crop the area, apply blur, and paste back\n",
    "    cropped_area = image.crop((x1, y1, x2, y2))\n",
    "    blurred_area = cropped_area.filter(ImageFilter.BoxBlur(radius=blur_radius))\n",
    "    image.paste(blurred_area, (x1, y1, x2, y2))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "51a6e6dd-fe99-4bfa-93c4-165595bcf104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './data/ds2_dense/images/lg-102414375-aug-beethoven--page-2.png'\n",
    "obboxes = train_data_agg_final[train_data_agg_final['filename']=='lg-102414375-aug-beethoven--page-2.png']['o_bbox']\n",
    "obboxes = obboxes.values[0]\n",
    "blurred_image = apply_random_blur(image_path)\n",
    "show_image_with_boxes(blurred_image, obboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6020b-6067-45d9-a807-81a81ecf28ec",
   "metadata": {},
   "source": [
    "### random zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "daf985d5-1321-4871-843d-db1a85029529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_zoom(image_path_or_bin, bounding_boxes, min_zoom=0.8, max_zoom=1.2):\n",
    "    \n",
    "    # Load the image\n",
    "    if type(image_path_or_bin) is str:\n",
    "        image = Image.open(image_path_or_bin)\n",
    "    else:\n",
    "        image = image_path_or_bin\n",
    "        \n",
    "    original_width, original_height = image.size\n",
    "    scale_factor = np.random.uniform(min_zoom, max_zoom)\n",
    "\n",
    "    # New dimensions\n",
    "    new_width, new_height = int(original_width * scale_factor), int(original_height * scale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    # Calculate the new image's padding offsets\n",
    "    pad_width = (original_width - new_width) // 2\n",
    "    pad_height = (original_height - new_height) // 2\n",
    "\n",
    "    # Create a new image with a white background\n",
    "    result_image = Image.new('L', (original_width, original_height), 'white')\n",
    "    result_image.paste(resized_image, (pad_width, pad_height))\n",
    "\n",
    "    # Adjust bounding boxes\n",
    "    adjusted_bboxes = []\n",
    "    if len(bounding_boxes[0])==8:\n",
    "        for coords in bounding_boxes:\n",
    "            adjusted_coords = []\n",
    "            for i in range(0, len(coords), 2):\n",
    "                new_x = coords[i] * scale_factor + pad_width\n",
    "                new_y = coords[i+1] * scale_factor + pad_height\n",
    "                adjusted_coords.extend([new_x, new_y])\n",
    "            adjusted_bboxes.append(tuple(adjusted_coords))\n",
    "    else:\n",
    "        for x1, y1, x2, y2 in bounding_boxes:\n",
    "            new_x1 = x1 * scale_factor + pad_width\n",
    "            new_y1 = y1 * scale_factor + pad_height\n",
    "            new_x2 = x2 * scale_factor + pad_width\n",
    "            new_y2 = y2 * scale_factor + pad_height\n",
    "            adjusted_bboxes.append((new_x1, new_y1, new_x2, new_y2))\n",
    "\n",
    "    return result_image, adjusted_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ee06d655-0cd4-4e32-83c1-351c88df70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './data/ds2_dense/images/lg-102414375-aug-beethoven--page-2.png'\n",
    "bboxes = train_data_agg_final[train_data_agg_final['filename']=='lg-102414375-aug-beethoven--page-2.png']['a_bbox']\n",
    "bboxes = bboxes.values[0]\n",
    "zoomed_image, zoomed_bboxes = random_zoom(image_path, obboxes)\n",
    "show_image_with_boxes(zoomed_image, zoomed_bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6fcef-1f6c-4db5-a9fa-9c355f2268d6",
   "metadata": {},
   "source": [
    "### image rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "923462c0-562a-4de2-aef3-c3298d7526a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image_and_boxes(image_path_or_bin, bounding_boxes, max_rotation_deg=5):\n",
    "    # Load the image\n",
    "    if type(image_path_or_bin) is str:\n",
    "        image = Image.open(image_path_or_bin)\n",
    "    else:\n",
    "        image = image_path_or_bin\n",
    "    \n",
    "    width, height = image.size\n",
    "\n",
    "    # Random rotation angle between -5 and 5 degrees\n",
    "    angle = np.random.uniform(-max_rotation_deg, max_rotation_deg)\n",
    "\n",
    "    # Rotate image with a white background\n",
    "    rotated_image = image.rotate(angle, expand=True, fillcolor='white')  # Ensure background is white\n",
    "\n",
    "    # Crop the image to the original dimensions\n",
    "    new_width, new_height = rotated_image.size\n",
    "    left = (new_width - width) // 2\n",
    "    top = (new_height - height) // 2\n",
    "    right = left + width\n",
    "    bottom = top + height\n",
    "    rotated_image = rotated_image.crop((left, top, right, bottom))\n",
    "\n",
    "    # Calculate new bounding boxes\n",
    "    new_bounding_boxes = []\n",
    "    rad_angle = math.radians(-angle)  # Negative to rotate the points back\n",
    "    for box in bounding_boxes:\n",
    "        new_box = []\n",
    "        if len(bounding_boxes[0])==8:\n",
    "            points = [(box[i], box[i + 1]) for i in range(0, len(box), 2)]\n",
    "        else:\n",
    "            points = [(box[0], box[1]), (box[2], box[1]), (box[2], box[3]), (box[0], box[3])]\n",
    "\n",
    "        for x, y in points:\n",
    "            # Translate point to origin\n",
    "            tx = x - width / 2\n",
    "            ty = y - height / 2\n",
    "            # Rotate point\n",
    "            new_x = (math.cos(rad_angle) * tx - math.sin(rad_angle) * ty) + width / 2\n",
    "            new_y = (math.sin(rad_angle) * tx + math.cos(rad_angle) * ty) + height / 2\n",
    "            new_box.extend([new_x, new_y])\n",
    "\n",
    "        if len(bounding_boxes[0])==8:\n",
    "            new_bounding_boxes.append(new_box)  # Store the four corner points for OBBs\n",
    "        else:\n",
    "            # Convert back to bounding box format\n",
    "            min_x, min_y = min(new_box[::2]), min(new_box[1::2])\n",
    "            max_x, max_y = max(new_box[::2]), max(new_box[1::2])\n",
    "            new_bounding_boxes.append((min_x, min_y, max_x, max_y))\n",
    "\n",
    "    return rotated_image, new_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c67903c8-4009-4fda-9534-b32f428ca2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './data/ds2_dense/images/lg-102414375-aug-beethoven--page-2.png'\n",
    "obboxes = train_data_agg_final[train_data_agg_final['filename']=='lg-102414375-aug-beethoven--page-2.png']['o_bbox']\n",
    "obboxes = obboxes.values[0]\n",
    "rotated_image, new_boxes = rotate_image_and_boxes(image_path, obboxes)\n",
    "show_image_with_boxes(rotated_image, new_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a35083-73d9-412d-bab6-41bfd7b1c709",
   "metadata": {},
   "source": [
    "#### blur + rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cd4b3d23-84d1-4958-9eb6-f1846154edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './data/ds2_dense/images/lg-102414375-aug-beethoven--page-2.png'\n",
    "obboxes = train_data_agg_final[train_data_agg_final['filename']=='lg-102414375-aug-beethoven--page-2.png']['o_bbox']\n",
    "obboxes = obboxes.values[0]\n",
    "rotated_image, new_boxes = rotate_image_and_boxes(image_path, obboxes)\n",
    "blurred_image = apply_random_blur(rotated_image)\n",
    "show_image_with_boxes(blurred_image, new_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03175941-07d7-46de-a2f3-4ed5eee6eb9b",
   "metadata": {},
   "source": [
    "### warp - parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e6d7a768-62a7-4f5d-989f-50fcdc4634d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_warp_to_image_and_boxes(image_path_or_bin, bounding_boxes, \n",
    "                                  horizontal_warp_range=0.05, vertical_warp_range=0.05):\n",
    "    # Load the image\n",
    "    if type(image_path_or_bin) is str:\n",
    "        image = cv2.imread(image_path_or_bin)\n",
    "    else: # convert PIL to CV2\n",
    "        image_array = np.array(image_path_or_bin.convert('RGB'))\n",
    "        image = image_array[:, :, ::-1]\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Randomly select warp ratios within the specified ranges\n",
    "    horizontal_warp = np.random.uniform(-horizontal_warp_range, horizontal_warp_range)\n",
    "    vertical_warp = np.random.uniform(-vertical_warp_range, vertical_warp_range)\n",
    "\n",
    "    # Define source points (original corners of the image)\n",
    "    src_points = np.float32([[0, 0], [width, 0], [0, height], [width, height]])\n",
    "\n",
    "    # Define destination points for warping\n",
    "    dst_points = np.float32([\n",
    "        [0, 0], \n",
    "        [width + horizontal_warp * width, vertical_warp * height], \n",
    "        [0, height - vertical_warp * height], \n",
    "        [width, height]\n",
    "    ])\n",
    "\n",
    "    # Compute the perspective transform matrix\n",
    "    matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "\n",
    "    # Warp the image using the transformation matrix\n",
    "    warped_image = cv2.warpPerspective(image, matrix, (width, height), borderMode=cv2.BORDER_CONSTANT, borderValue=(255, 255, 255))\n",
    "    warped_image = Image.fromarray(warped_image).convert('L')\n",
    "    \n",
    "    # Adjust bounding boxes\n",
    "    new_bounding_boxes = []\n",
    "    for box in bounding_boxes:\n",
    "        new_box = []\n",
    "        # Transform each corner of the bounding box\n",
    "        for i in range(0, len(box), 2):\n",
    "            point = np.array([[[box[i], box[i+1]]]], dtype='float32')\n",
    "            # Apply the transformation matrix\n",
    "            transformed_point = cv2.perspectiveTransform(point, matrix)\n",
    "            new_box.extend(transformed_point[0][0])\n",
    "        if not len(bounding_boxes[0])==8:\n",
    "            # For non-oriented boxes, recalculate to orthogonal bounds\n",
    "            min_x, min_y = min(new_box[::2]), min(new_box[1::2])\n",
    "            max_x, max_y = max(new_box[::2]), max(new_box[1::2])\n",
    "            new_box = [min_x, min_y, max_x, max_y]\n",
    "        new_bounding_boxes.append(new_box)\n",
    "\n",
    "    return warped_image, new_bounding_boxes # this is a PIL image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5c1b3ba5-4153-4306-baca-af0aff98b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './data/ds2_dense/images/lg-102414375-aug-beethoven--page-2.png'\n",
    "bboxes = train_data_agg_final[train_data_agg_final['filename']=='lg-102414375-aug-beethoven--page-2.png']['a_bbox']\n",
    "bboxes = bboxes.values[0]\n",
    "warped_image, new_bounding_boxes = apply_warp_to_image_and_boxes(image_path, bboxes)\n",
    "show_image_with_boxes(warped_image, new_bounding_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec34870-7cc4-4e0e-a58c-366d31bb48ee",
   "metadata": {},
   "source": [
    "### warp - trapezoidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3d55d22e-6b9a-4e2d-b01d-ead204d5d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_keystone_warp_to_image_and_boxes(image_path_or_bin, bounding_boxes, max_skew_factor=0.05):\n",
    "    # Load the image\n",
    "    if type(image_path_or_bin) is str:\n",
    "        image = cv2.imread(image_path_or_bin)\n",
    "    else: # convert PIL to CV2\n",
    "        image_array = np.array(image_path_or_bin.convert('RGB'))\n",
    "        image = image_array[:, :, ::-1]\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Define source points (original corners of the image)\n",
    "    src_points = np.float32([[0, 0], [width, 0], [0, height], [width, height]])\n",
    "\n",
    "    # Randomly apply skew factors to each corner\n",
    "    dst_points = np.float32([\n",
    "        [np.random.uniform(-max_skew_factor, max_skew_factor) * width, \n",
    "         np.random.uniform(-max_skew_factor, max_skew_factor) * height],\n",
    "        [width + np.random.uniform(-max_skew_factor, max_skew_factor) * width, \n",
    "         np.random.uniform(-max_skew_factor, max_skew_factor) * height],\n",
    "        [np.random.uniform(-max_skew_factor, max_skew_factor) * width, \n",
    "         height + np.random.uniform(-max_skew_factor, max_skew_factor) * height],\n",
    "        [width + np.random.uniform(-max_skew_factor, max_skew_factor) * width, \n",
    "         height + np.random.uniform(-max_skew_factor, max_skew_factor) * height]\n",
    "    ])\n",
    "\n",
    "    # Compute the perspective transform matrix\n",
    "    matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "\n",
    "    # Warp the image using the transformation matrix\n",
    "    warped_image = cv2.warpPerspective(image, matrix, (width, height), \n",
    "                                       borderMode=cv2.BORDER_CONSTANT, \n",
    "                                       borderValue=(255, 255, 255))\n",
    "\n",
    "    # Convert warped image back to PIL format and convert to grayscale\n",
    "    warped_image_pil = Image.fromarray(warped_image[:, :, ::-1]).convert('L')\n",
    "\n",
    "    # Adjust bounding boxes\n",
    "    new_bounding_boxes = []\n",
    "    for box in bounding_boxes:\n",
    "        transformed_points = []\n",
    "        # Transform each corner of the bounding box\n",
    "        for i in range(0, len(box), 2):\n",
    "            point = np.array([[[box[i], box[i + 1]]]], dtype='float32')\n",
    "            transformed_point = cv2.perspectiveTransform(point, matrix)\n",
    "            transformed_points.extend(transformed_point[0][0])\n",
    "\n",
    "        # Recalculate the bounding box to contain all points\n",
    "        xs = transformed_points[0::2]\n",
    "        ys = transformed_points[1::2]\n",
    "        new_box = [min(xs), min(ys), max(xs), max(ys)]\n",
    "        new_bounding_boxes.append(new_box)\n",
    "\n",
    "    return warped_image_pil, new_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ef470d6a-1ab3-4204-a14f-5c59a3bcf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = './data/ds2_dense/images/lg-102414375-aug-beethoven--page-2.png'\n",
    "obboxes = train_data_agg_final[train_data_agg_final['filename']=='lg-102414375-aug-beethoven--page-2.png']['o_bbox']\n",
    "obboxes = obboxes.values[0]\n",
    "twarped_image, new_bounding_boxes = apply_keystone_warp_to_image_and_boxes(image_path, obboxes)\n",
    "show_image_with_boxes(twarped_image, new_bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec1dc83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
